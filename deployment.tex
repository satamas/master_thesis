
\chapter{Инфраструктура}

В данной главе подробно рассмотрены детали масштабируемой архитектуры приложения.

\section{Внутренние сервера}
\subsection{Старт нового сервера}
	При автоматическом запуске машины на Amazon мы можем передать очень маленькое количество информации, а именно, идентификатор AMI и User Data - небольшой набор инструкций, который будет выполнен после запуска. Данный параграф посвящён тому, как при помощи этого небольшого количества данных можно настроить компьютер и запустить приложение.
	
	Для того, чтобы просто запустить компьютер нам достаточно идентификатора AMI. Первоначально нами был выбран образ с CoreOS, т.к. планировалось запускать приложение внутри докер-контейнера, а CoreOS как раз предназначена для таких случаев.
	
	После того, как мы запустили компьютер нам необходимо скачать наше приложение и всю необходимую инфраструктуру, используя при этом только докер, т.к. в CoreOS кроме него ничего нет.
	
	Теоретически, всё приложение вместе с инфраструктурой можно было бы поместить внутрь одного докер образа, а при старте машины запустить докер-контейнер, основанный на нём. Проблема такого подхода заключается в том, что при обновлении исходного кода приложения, что происходит очень часто, нам приходилось бы добавлять слой, что приводило бы к сильному росту размера образа.
	
	С учётом этого был применён несколько другой подход, при котором вся необходимая инфраструктура и настройки, которые меняются достаточно редко, помещаются в докер образ, а само приложение хранится в S3Bucket - облачном хранилище Amazon. В такой ситуации при старте машины нам необходимо  скачать наше приложение из S3 при помощи соответствующего приложения, запущенного внутри докер контейнера. После этого мы можем запускать основной контейнер, который основывается на образе с настройками, и передать ему скачанное приложение.
	
	Все необходимые докер-контейнеры запускаются в виде системного сервиса (см. systemd). Это позволяет сохранить информацию об ошибках, возникших при исполнении контейнеров. Также при таком подходе контейнер с приложением может быть автоматически перезапущен в случае если его исполнение завершилось с ошибкой.
	
	В результате получается следующая User Data:
\begin{lstlisting}
[Unit]
Description=Kotlin web Demo backend Green
After=docker.service

[Service]
TimeoutStartSec=1800s
Restart=always
ExecStartPre=-/usr/bin/docker rm -f war-container-green
ExecStartPre=/usr/bin/docker run --name=war-container-green -v /wars docker-registry.labs.intellij.net/itops/aws-cli s3 cp s3://kotlin-web-demo-backend/green/WebDemoBackend.war /wars
ExecStartPre=/usr/bin/docker pull docker-registry.labs.intellij.net/kotlin/web-demo-backend
ExecStart=/usr/bin/docker run -m="1536m" --rm=true -e "LFS_NAME=kotlin.web.demo.backend" --name tomcat-green --volumes-from war-container-green -p 8080:20039 docker-registry.labs.intellij.net/kotlin/web-demo-backend

ExecStop=/usr/bin/docker stop tomcat-green
\end{lstlisting}

\subsection{Выбор типа компьютера}
	%TODO: всюду "тип"
	Одним из параметров конфигурации запуска компьютера на Amazon является его тип. Он определяет то, насколько мощный будет CPU, количество памяти и.т.д.. Также от него зависит стоимость -- чем лучше параметры, тем дороже компьютер стоит, что делает выбор типа компьютера важным этапом создания инфраструктуры на Amazon.
	
	В таблице \ref{table:instance_types} приведены основные характеристики тех типов, которые рассматривались в качестве кандидатов.
	
	Как видно, CPU у ряда типов компьютеров, а именно, у компьютеров типа t2, помечены Burstable. Это -- возможность временно увеличивать производительность CPU, тратя при этом кредиты CPU. Кредиты CPU -- это абстракция, использующаяся Amazon. Они копятся со временем и теряются либо при большом потреблении CPU, либо по истечении суток. Подобная система разработана для приложений, у которых в среднем нагрузка мала, однако периодически она может возрастать.
	
\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c|c}
		Type      & CPU Units    & Memory & Cost\\ \hline
		t2.small  & 1(Burstable) & 2GB    & \$0.026 hourly\\ \hline
		t2.medium & 2(Burstable) & 4GB    & \$0.052 hourly\\ \hline
		m3.medium & 3            & 3.75GB & \$0.070 hourly\\ \hline
		c4.large  & 8            & 3.75GB & \$0.116 hourly\\
	\end{tabular}
	\caption{Параметры интересующих нас типов амазоновских инстансов}
	\label{table:instance_types}
\end{table}
	
	Для сравнения производительности компьютеров использовались два простых нагрузочных теста. В обоих тестах на сервер посылались запросы на исполнение ``Hello, World!'' программы. В первом случае запросы постоянно отправлялись из одного потока, и измерялось время обработки каждого запроса. Во втором случае измерялось максимальное количество потоков, при котором сервер успевал исполнять программу (через 5 секунд после старта программа завершается).
	
\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c}
		Type      & Время обработки запроса    & Максимальное число потоков\\ \hline
		t2.small  & 0.5(Burst)/2.5             & 7(Burst)/1  \\ \hline
		t2.medium & 0.5(Burst)/2.5             & 15(Burst)/2 \\ \hline
		m3.medium & 1.2                        & 4           \\ \hline
		c4.large  & 0.5                        & 10          \\
	\end{tabular}
	\caption{Параметры интересующих нас типов амазоновских инстансов}
	\label{table:instance_types_performance}
\end{table}

	Из двух приведённых выше таблиц видно, что нам явно не подходят m3.medium инстансы, т.к. у них стабильно низкая мощность CPU.
	
	Намного интересней всё с компьютерами типа t2. Из таблицы \ref{table:instance_types_performance}	 видно, что они могут выдавать производительность даже большую, чем более дорогие компьютеры типа c4.large, но есть очевидная проблема, которая заключается в том, что мы можем потратить все CPU кредиты. Данная проблема наиболее существенна при возникновении достаточно долгой большой нагрузки, т.к. общего числа накопленных кредитов может хватить максимум на час работы при максимальном потреблении CPU. В такой ситуации один компьютер перестанет справляться, и система автоматического масштабирования должна будет поднять новые, которые тоже будут типа t2. А это означает, что каждая поднятая машина в таких условиях сможет нормально работать только в течение часа, после чего она станет почти бесполезной. 
	
	Также существует ещё одна связанная с кредитами проблема. Ресурсом, который мы активней всего тратим, является CPU, поэтому и наши метрики, которые отвечают за масштабирование, настроены на то, чтобы смотреть на потребление CPU. Когда у t2 компьютера кончаются кредиты уровень потребления CPU у него не поднимается выше 20\%. Это приводит к тому, что согласно нашим метрикам, нагрузка на систему мала, а значит и новые компьютеры поднимать не нужно. Данная ситуация является катастрофичной, т.к. сервера не смогут обрабатывать запросы в связи с тем, что они перегружены, а система масштабирования будет считать что всё нормально и ничего не предпринимать, что приведёт к неработоспособности приложения до тех пор, пока не спадёт нагрузка.
	
	Учитывая всё вышесказанное, можно сказать, что компьютеры типа t2 обладают очень хорошим соотношением мощности к цене, однако в сколь бы то ни было серьёзном приложении их использование представляется невозможным в связи с непредсказуемостью поведения. Учитывая это, а так же тот факт , что тип m3.medium обладает очень слабым процессором, в итоге были выбран тип компьютеров с4.large.
\subsection{Подбор метрик}
	Как было сказано ранее, автоматическое масштабирование в амазоне основано на событиях Cloud Watch Alarm, каждое из которых говорит о том, что потребление определённого ресурса было больше/меньше определённого уровня.
	
	%TODO: либо "учитывая это, мы сделали ...", либо "с учётом этого было сделано ... "
	Как показал мониторинг нашего приложения, ресурс, который мы можем целиком потратить, один - процессорное время. С учётом этого были созданы соответствующие события:
\begin{itemize}
	\item Если среднее потребление CPU больше 80\% в течение 2 минут, то увеличить количество инстансов в 2 раза, после чего подождать одну минуту перед следующим масштабированием.
	\item Если среднее потребление CPU меньше 40\% в течение часа, то 
\end{itemize}

	При срабатывании первого события система атвоматического масштабирования должна увеличить количество компьютеров в 2 раза, после чего в течение минуты не предпринимать никакой активности.
	
	При срабатывании второго события система должна уменьшить количество компьютеров на 1, после чего подождать пять минут перед следующим масштабированием.

	Из предыдущих абзацев видно, что существенно различается время, которое мы ждём, перед тем как запускать новые компьютеры и перед тем как уничтожать старые. Это позволяет нам избежать ситуации, при которой мы будем останавливать только что запущенные компьютеры из-за того, что немного уменьшилась нагрузка, а потом запускать их обратно, т.к. нагрузка обратно возросла. Кроме того, при старте новой машины на амазоне мы оплачиваем один час её работы, что делает подобный подход ещё более осмысленным.
	
	Выбранная нами политика масштабирования является очень агрессивной, т.е. она очень мало ждёт перед поднятием новых компьютеров и увеличивает их количество в геометрической прогрессии. Это сделано для того, чтобы мы могли оперативно среагировать на резко увеличившуюся нагрузку. Недостатком такого подхода является то, что при случайном превышении порога, мы можем поднять много лишних машин, но в случае нашего приложения этот недостаток не проявляется, т.к. средняя нагрузка на наше приложение сильно меньше порога.
	
\subsection{Время запуска машины. Использование AMI}
	Время отклика приложения на рост нагрузки зависит не только от того, как мы настроили события в Cloud Watch, но и от того, как долго будет запускаться новый компьютер и сколько времени будет настраиваться на нём наша инфраструктура. 
	
	Со временем запуска новой машины мы сделать ничего не можем, это время более-менее постоянно и равно 1-2 минутам, а вот время настройки инфраструктуры мы можем пытаться уменьшить. В первоначальной версии это время составляло порядка 7 минут, что приводило к тому, что суммарное время отклика на рост нагрузки было больше 10 минут (2 минуты для срабатывания события, 2 минуты чтобы запустить машину, 7 минут чтобы настроить инфраструктуру). Основную часть этого времени мы тратили на скачивание образов докера, среди которых есть очень объёмный образ операционной системы.
	
	Единственный способ избежать скачивания докер-образов -- это иметь эти докер образы локально. Это можно сделать, если создать образ работающей машины и использовать его для запуска новых машин. При таком подходе время настройки нашей инфраструктуры удалось сократить до нескольких минут, что привело к суммарному времени порядка 5 минут. 
\section{Инфраструктура с учётом обновлений}
\subsection{Green Blue deployment}
	Среди многочисленных подходов к тому, как обновлять приложение, нами был выбран ``Blue Green Deployment'', при котором существует две версии приложения, одна из которых является активной. Схема данного подхода в случае нашего приложения приведена на рис.\ref{fig:green_blue_deployment}.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{green_blue_deployment} 
    \caption{Детальная схема устройства нашего приложения. Зелёным отмечено активное приложение, синим -- пассивное}
    \label{fig:green_blue_deployment}
\end{figure}

	Какое приложение является на данный момент активным определяется в нашей схеме настройками nginx сервера, который принимает все запросы извне. Если запрос был отправлен на публичный адрес try.kotlinlang.org, то он будет перенаправлен активному приложению, а если он был отправлен на адрес, который известен только разработчикам, то он будет перенаправлен в пассивную ветку приложения.
	
	При публикации приложения оно сперва выкладывается в пассивную ветку, после чего проверяется его работоспособность. Если приложение работоспособно, то далее меняются настройки nginx-сервера  и обновлённое приложение становится активным, что завершает процесс публикации обновлений.
\subsection{Выбор стоимость-надёжность}
	Как видно из рисунка .\ref{fig:green_blue_deployment}, мы не дублируем никакие машины кроме ELB, которые необходимо дублировать, т.к. они умеют слушать только один порт и отправлять запросы тоже только на один порт. Такая схема является экономной, однако имеет свои недостатки.
	
	Основным минусом такой схемы является то, что при возникновении проблем в обновлённой версии приложения могут возникнуть проблемы в активном приложении.
	
	На внешнем сервере подобной проблемы нет, т.к. каждое из приложений работает внутри своего докер-контейнера, и если в одном из них происходит ошибка, то второе ничего не замечает, т.к. они работают в изолированном окружении.
	
	На внутренних серверах всё было бы точно так же, если бы внутренние сервера не проверялись постоянно на работоспособность балансировщиком нагрузки. Это приводит к тому, что если одно из наших приложений не отвечает по какой-то причине (что-то зависло в процессе публикации, мы выложили плохое приложение и.т.д), то данный компьютер будет остановлен, а на смену ему будет поднят новый. В худшем случае с новым компьютером всё может повториться и система автоматического масштабирования так и будет запускать и останавливать новые компьютеры, пока мы не починим приложение.
	
	Самым простым решением данной проблемы является копирование инфраструктуры для пассивного приложения, но в такой ситуации нужно тратить дополнительные деньги на работу инстансов пассивной ветки.